{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "* [ ] create GD/SGD\n",
    "* [ ] Score w/ loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tune import Damper\n",
    "from pprint import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:53516</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>17.18 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:53516' processes=4 threads=8, memory=17.18 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from distributed import Client, LocalCluster\n",
    "# client = await Client(\"localhost:6786\", asynchronous=True)\n",
    "# cluster = await LocalCluster(n_workers=3, threads_per_worker=2, asynchronous=True)\n",
    "# client = await Client(cluster)\n",
    "# client = await Client(\"localhost:8786\", asynchronous=True)\n",
    "client = await Client(asynchronous=True)\n",
    "# client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "await client.upload_file(\"train.py\")\n",
    "await client.upload_file(\"tune.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test():\n",
    "    from tune import Damper\n",
    "    d = Damper()\n",
    "    return True\n",
    "\n",
    "f = client.submit(test)\n",
    "res = await client.gather(f)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_covtype(return_X_y=True, shuffle=False, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581012, 54)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly = kernel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581012, 1485)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_poly, y, random_state=42, train_size=200_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.212822437286377"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.nbytes / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 1485)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 613)\n",
      "(200000, 613)\n"
     ]
    }
   ],
   "source": [
    "def _get_cnts_cols(X):\n",
    "    cols = range(X.shape[1])\n",
    "    uniqs = [np.unique(X[:, c]) for c in cols]\n",
    "    cnts = [c for c, _u in zip(cols, uniqs) if len(_u) > 2]\n",
    "    discrete_cols = [c for c, _u in zip(cols, uniqs) if len(_u) == 2]\n",
    "    return cnts, discrete_cols\n",
    "\n",
    "def normalize(X, scale, cnts, discrete):\n",
    "    Y = scale.transform(X[:, cnts])\n",
    "    Y2 = X[:, discrete].astype(bool).astype(int)  # one element is 30 (not 0/1)\n",
    "    Z = np.hstack((Y2, Y))\n",
    "    return Z\n",
    "\n",
    "cnts, discrete = _get_cnts_cols(X_train)\n",
    "scale = StandardScaler().fit(X_train[:, cnts])\n",
    "\n",
    "print(X_train[:, cnts + discrete].shape)\n",
    "X_train = normalize(X_train, scale, cnts, discrete)\n",
    "X_test = normalize(X_test, scale, cnts, discrete)\n",
    "print(X_train.shape)\n",
    "\n",
    "uniqs = np.unique(X_train[:, :len(discrete)])\n",
    "assert len(uniqs) == 2 and 0 <= uniqs.min() <= uniqs.max() <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 613)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381012, 613)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform, uniform\n",
    "from copy import copy\n",
    "\n",
    "batch_pow = np.arange(5, 9 + 1).astype(int)\n",
    "static_batch = (2 ** batch_pow).astype(int).tolist()\n",
    "\n",
    "ibs = (2 ** batch_pow).astype(int).tolist()\n",
    "mbs = (2 ** (batch_pow + 1)).astype(int).tolist()\n",
    "\n",
    "lrs = loguniform(0.5e-4, 1e-1)\n",
    "weight_decays = loguniform(1e-8, 1e-2)\n",
    "momentums = uniform(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = {\n",
    "    \"max_batch_size\": static_batch,  # 5 choices\n",
    "    \"weight_decay\": weight_decays,  # cnts\n",
    "    \"momentum\": momentums,  # cnts\n",
    "    \"lr\": [0.9e-3],\n",
    "    \"scoring\": [\"loss\"],\n",
    "}\n",
    "\n",
    "dwell = [1, 2, 5, 10, 20, 50, 100, 200]\n",
    "padadamp_params = {\n",
    "    **copy(base),\n",
    "    \"batch_growth_rate\": loguniform(1e-3, 1e-1),\n",
    "    \"dwell\": dwell,\n",
    "    \"initial_batch_size\": ibs,\n",
    "    \"max_batch_size\": mbs,\n",
    "}\n",
    "\n",
    "hsgd_params = {\n",
    "    **copy(base),\n",
    "    \"initial_batch_size\": [256],\n",
    "    \"max_batch_size\": [2048],\n",
    "    \"lr\": [0.9e-3],\n",
    "    \"batch_growth_rate\": loguniform(1e-3, 1e-1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_params = {\n",
    "    **copy(base),\n",
    "    \"max_batch_size\": [len(X_train)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_batch_size': [200000],\n",
       " 'weight_decay': <scipy.stats._distn_infrastructure.rv_frozen at 0x129f319d0>,\n",
       " 'momentum': <scipy.stats._distn_infrastructure.rv_frozen at 0x11b46b1c0>,\n",
       " 'lr': [0.0009],\n",
       " 'scoring': ['loss']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_examples = 75 * len(X_train)\n",
    "n_params = 25\n",
    "\n",
    "max_iter = n_params\n",
    "chunk_size = np.round(n_examples / n_params) + 0\n",
    "chunk_size / len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_examples = 200 * len(X_train)\n",
    "n_params = 200\n",
    "\n",
    "max_iter = n_params\n",
    "chunk_size = np.round(n_examples / n_params) + 0\n",
    "chunk_size = min(len(X_train), chunk_size)\n",
    "chunk_size / len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.array as da\n",
    "# max_iter = 27\n",
    "# chunk_size = 10_000\n",
    "\n",
    "# n = 3000\n",
    "# X_train = np.random.uniform(size=(n, 54))\n",
    "# y_train = np.random.choice(8, size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000,), (613,))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.array as da\n",
    "X_train2 = da.from_array(X_train, chunks=(chunk_size, -1))\n",
    "y_train2 = da.from_array(y_train, chunks=chunk_size)\n",
    "\n",
    "X_train2.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train2.persist()\n",
    "y_train2 = y_train2.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-beceef0fd9a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "del X_test\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tune import PadaDamp, Damper, GD, HSGD\n",
    "\n",
    "seed = 33\n",
    "\n",
    "padadamp = PadaDamp(seed=seed)\n",
    "hsgd = HSGD(seed=seed, name=\"hsgd\")\n",
    "gd = GD(seed=seed, name=\"gd\")\n",
    "sgd = Damper(seed=seed, name=\"sgd\")\n",
    "asgd = Damper(seed=seed, name=\"asgd\", opt=\"asgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4912"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "p2 = clone(padadamp)\n",
    "p2.initialize()\n",
    "n_params = [v.nelement() for v in p2.model.module_.parameters()]\n",
    "sum(n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import HyperbandSearchCV\n",
    "\n",
    "kwargs = dict(\n",
    "    max_iter=max_iter, verbose=True, random_state=6, patience=2, tol=None,\n",
    ")\n",
    "\n",
    "models = {\n",
    "#     \"pada\": (padadamp, padadamp_params),\n",
    "#     \"gd\": (gd, gd_params),\n",
    "#     \"asgd\": (sgd, base),\n",
    "    \"hsgd\": (hsgd, hsgd_params),\n",
    "}\n",
    "\n",
    "searches = {\n",
    "    name: HyperbandSearchCV(model, params, prefix=f\"-{name}\", **kwargs)\n",
    "    for name, (model, params) in models.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'partial_fit_calls': 173, 'n_models': 17}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _search = searches[\"pada\"]\n",
    "_search = searches[\"hsgd\"]\n",
    "{k: v for k, v in _search.metadata.items() if not isinstance(v, list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 613)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs = {name: search.fit(X_train2, y_train2) for name, search in searches.items()}\n",
    "import asyncio\n",
    "jobs = [\n",
    "    search.fit(X_train2, y_train2) if name != \"gd\" else search.fit(X_train, y_train)\n",
    "    for name, search in searches.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV-geo, bracket=5] creating 243 models\n",
      "[CV-geo, bracket=4] creating 98 models\n",
      "[CV-geo, bracket=3] creating 41 models\n",
      "[CV-geo, bracket=2] creating 18 models\n",
      "[CV-geo, bracket=1] creating 9 models\n",
      "[CV-geo, bracket=0] creating 6 models\n",
      "[CV-pada, bracket=5] creating 243 models\n",
      "[CV-pada, bracket=4] creating 98 models\n",
      "[CV-pada, bracket=3] creating 41 models\n",
      "[CV-pada, bracket=2] creating 18 models\n",
      "[CV-pada, bracket=1] creating 9 models\n",
      "[CV-pada, bracket=0] creating 6 models\n",
      "[CV-gd, bracket=5] creating 243 models\n",
      "[CV-gd, bracket=4] creating 98 models\n",
      "[CV-gd, bracket=3] creating 41 models\n",
      "[CV-gd, bracket=2] creating 18 models\n",
      "[CV-gd, bracket=1] creating 9 models\n",
      "[CV-gd, bracket=0] creating 6 models\n",
      "[CV-sgd, bracket=5] creating 243 models\n",
      "[CV-sgd, bracket=4] creating 98 models\n",
      "[CV-sgd, bracket=3] creating 41 models\n",
      "[CV-sgd, bracket=2] creating 18 models\n",
      "[CV-sgd, bracket=1] creating 9 models\n",
      "[CV-sgd, bracket=0] creating 6 models\n",
      "[CV-adagrad, bracket=5] creating 243 models\n",
      "[CV-adagrad, bracket=4] creating 98 models\n",
      "[CV-adagrad, bracket=3] creating 41 models\n",
      "[CV-adagrad, bracket=2] creating 18 models\n",
      "[CV-adagrad, bracket=1] creating 9 models\n",
      "[CV-adagrad, bracket=0] creating 6 models\n",
      "[CV-geo, bracket=3] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-pada, bracket=1] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-pada, bracket=0] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-pada, bracket=2] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-geo, bracket=2] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-geo, bracket=1] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-geo, bracket=0] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-pada, bracket=3] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-pada, bracket=0] validation score of -0.6621 received after 1 partial_fit calls\n",
      "[CV-pada, bracket=1] validation score of -0.6446 received after 1 partial_fit calls\n",
      "[CV-geo, bracket=1] validation score of -0.6582 received after 1 partial_fit calls\n",
      "[CV-geo, bracket=2] validation score of -0.6578 received after 1 partial_fit calls\n",
      "[CV-pada, bracket=2] validation score of -0.6522 received after 1 partial_fit calls\n",
      "[CV-geo, bracket=3] validation score of -0.6499 received after 1 partial_fit calls\n",
      "[CV-gd, bracket=4] For training there are between 80000 and 80000 examples in each chunk\n",
      "[CV-adagrad, bracket=0] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-geo, bracket=0] validation score of -0.6810 received after 1 partial_fit calls\n",
      "[CV-adagrad, bracket=3] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-gd, bracket=2] For training there are between 80000 and 80000 examples in each chunk\n",
      "[CV-gd, bracket=1] For training there are between 80000 and 80000 examples in each chunk\n",
      "[CV-gd, bracket=5] For training there are between 80000 and 80000 examples in each chunk\n",
      "[CV-gd, bracket=3] For training there are between 80000 and 80000 examples in each chunk\n",
      "[CV-geo, bracket=4] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-adagrad, bracket=5] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-pada, bracket=3] validation score of -0.6394 received after 1 partial_fit calls\n",
      "[CV-adagrad, bracket=4] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-sgd, bracket=4] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-pada, bracket=5] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-pada, bracket=4] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-sgd, bracket=5] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-geo, bracket=5] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-sgd, bracket=3] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-sgd, bracket=0] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-sgd, bracket=2] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-adagrad, bracket=2] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-gd, bracket=4] validation score of -1.7652 received after 1 partial_fit calls\n",
      "[CV-sgd, bracket=1] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-gd, bracket=0] For training there are between 80000 and 80000 examples in each chunk\n",
      "[CV-adagrad, bracket=0] validation score of -0.6865 received after 1 partial_fit calls\n",
      "[CV-adagrad, bracket=1] For training there are between 40000 and 40000 examples in each chunk\n",
      "[CV-gd, bracket=1] validation score of -2.1239 received after 1 partial_fit calls\n",
      "[CV-gd, bracket=2] validation score of -1.8546 received after 1 partial_fit calls\n"
     ]
    }
   ],
   "source": [
    "running_searches = asyncio.gather(*jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_searches = running_searches.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HyperbandSearchCV(estimator=GeoDamp(seed=33), max_iter=300,\n",
       "                   parameters={'dampingdelay': [1, 2, 5, 10, 20, 50, 100],\n",
       "                               'dampingfactor': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1155e2640>,\n",
       "                               'initial_batch_size': [32, 64, 128, 256, 512],\n",
       "                               'lr': <scipy.stats._distn_infrastructure.rv_frozen object at 0x115734400>,\n",
       "                               'max_batch_size': [64, 128, 256, 512, 1024],\n",
       "                               'momentum': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1157347c0>,\n",
       "                               'scoring': ['loss'],\n",
       "                               'weight_decay': <scipy.stats._distn_infrastructure.rv_frozen object at 0x115734460>},\n",
       "                   patience=True, prefix='-geo', random_state=6, verbose=True),\n",
       " HyperbandSearchCV(estimator=PadaDamp(seed=33), max_iter=300,\n",
       "                   parameters={'batch_growth_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1155e26a0>,\n",
       "                               'dwell': [1, 2, 5, 10, 20, 50, 100, 200, 500,\n",
       "                                         1000],\n",
       "                               'initial_batch_size': [32, 64, 128, 256, 512],\n",
       "                               'lr': <scipy.stats._distn_infrastructure.rv_frozen object at 0x115734400>,\n",
       "                               'max_batch_size': [64, 128, 256, 512, 1024],\n",
       "                               'momentum': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1157347c0>,\n",
       "                               'scoring': ['loss'],\n",
       "                               'weight_decay': <scipy.stats._distn_infrastructure.rv_frozen object at 0x115734460>},\n",
       "                   patience=True, prefix='-pada', random_state=6, verbose=True),\n",
       " HyperbandSearchCV(estimator=AdaDamp(dataset=(array([[-0.08398266, -0.79771241,  0.25349172, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.69321561, -0.69059105, -0.4148299 , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.06921627, -1.18156395, -1.48414449, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [-1.29532302,  1.80890733,  0.52082036, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 1.25918005,  1.21081308, -1.21681584, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 1.28768217, -1.13693005,  0.11982739, ...,  0.        ,\n",
       "          1.        ,  0...\n",
       "                               'initial_batch_size': [32, 64, 128, 256, 512],\n",
       "                               'lr': <scipy.stats._distn_infrastructure.rv_frozen object at 0x115734400>,\n",
       "                               'max_batch_size': [64, 128, 256, 512, 1024],\n",
       "                               'momentum': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1157347c0>,\n",
       "                               'scoring': ['loss'],\n",
       "                               'weight_decay': <scipy.stats._distn_infrastructure.rv_frozen object at 0x115734460>},\n",
       "                   patience=True, prefix='-adadamp', random_state=6,\n",
       "                   verbose=True),\n",
       " HyperbandSearchCV(estimator=GD(name='gd', seed=33), max_iter=300,\n",
       "                   parameters={'lr': <scipy.stats._distn_infrastructure.rv_frozen object at 0x115734400>,\n",
       "                               'max_batch_size': [25000],\n",
       "                               'momentum': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1157347c0>,\n",
       "                               'scoring': ['loss'],\n",
       "                               'weight_decay': <scipy.stats._distn_infrastructure.rv_frozen object at 0x115734460>},\n",
       "                   patience=True, prefix='-gd', random_state=6, verbose=True),\n",
       " HyperbandSearchCV(estimator=Damper(name='sgd', seed=33), max_iter=300,\n",
       "                   parameters={'lr': <scipy.stats._distn_infrastructure.rv_frozen object at 0x115734400>,\n",
       "                               'max_batch_size': [32, 64, 128, 256, 512],\n",
       "                               'momentum': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1157347c0>,\n",
       "                               'scoring': ['loss'],\n",
       "                               'weight_decay': <scipy.stats._distn_infrastructure.rv_frozen object at 0x115734460>},\n",
       "                   patience=True, prefix='-sgd', random_state=6, verbose=True),\n",
       " HyperbandSearchCV(estimator=Damper(name='adagrad', opt='adagrad', seed=33),\n",
       "                   max_iter=300,\n",
       "                   parameters={'lr': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1155e25e0>,\n",
       "                               'max_batch_size': [32, 64, 128, 256, 512],\n",
       "                               'momentum': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1157347c0>,\n",
       "                               'scoring': ['loss'],\n",
       "                               'weight_decay': <scipy.stats._distn_infrastructure.rv_frozen object at 0x115734460>},\n",
       "                   patience=True, prefix='-adagrad', random_state=6,\n",
       "                   verbose=True)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finished_searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for finished_search in finished_searches:\n",
    "    name = \"data/\" + finished_search.prefix[1:]\n",
    "    pd.DataFrame(finished_search.cv_results_).to_csv(f\"{name}-cv-results.csv\", index=False)\n",
    "    pd.DataFrame(finished_search.history_).to_csv(f\"{name}-hist.csv\", index=False)\n",
    "    pd.DataFrame(finished_search.best_estimator_.history_).to_csv(f\"{name}-best-est-hist.csv\", index=False)\n",
    "    with open(f\"{name}-best-params.json\", \"w\") as f:\n",
    "        json.dump(finished_search.best_params_, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:skorch]",
   "language": "python",
   "name": "conda-env-skorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
