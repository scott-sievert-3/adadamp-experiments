{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate DaskClassifier based on Timing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import importlib\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from copy import copy\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import simulator\n",
    "os.chdir('..')\n",
    "classifier = importlib.import_module(\"exp-dask.classifier\")\n",
    "from classifier import DaskClassifierSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>inproc://192.168.0.107/32491/1</li>\n",
       "  <li><b>Dashboard: </b><a href='http://192.168.0.107:8787/status' target='_blank'>http://192.168.0.107:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>17.18 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'inproc://192.168.0.107/32491/1' processes=1 threads=8, memory=17.18 GB>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training client\n",
    "def _prep():\n",
    "    from distributed.protocol import torch\n",
    "\n",
    "client = Client(processes=False)\n",
    "client.run(_prep)\n",
    "\n",
    "from model import Wide_ResNet\n",
    "client.upload_file(\"./exp-dask/model.py\")\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Gather and prepare data\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "train_set = torchvision.datasets.CIFAR10(root='./exp-dask/data', train=True, download=True, transform=transform_train)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./exp-dask/data', train=False, download=True, transform=transform_test)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_stats(hist, directory, filename):\n",
    "    with open('./exp-dask/{}/{}'.format(directory, filename), 'w', encoding='utf8', newline='') as output_file:\n",
    "        fc = csv.DictWriter(output_file, fieldnames=hist[0].keys())\n",
    "        fc.writeheader()\n",
    "        fc.writerows(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, test_set, stats_df, n_epochs=200, log_interval=1, stats_dir=''):\n",
    "    \"\"\"\n",
    "    Train based on expiriment params\n",
    "    \"\"\"\n",
    "    \n",
    "    # stats to load into the simulator\n",
    "    stats = list(stats_df.T.to_dict().values())\n",
    "    print(\"[SETUP] Loaded {} epochs of stats\".format(len(stats)))\n",
    "    \n",
    "    # preprocess data\n",
    "    start = time()\n",
    "    train_set = model.preprocess(train_set)\n",
    "    test_set = model.preprocess(test_set)\n",
    "    print(\"[SETUP] Pre-Processed in {} seconds\".format(time() - start))\n",
    "    \n",
    "    history = []\n",
    "    # run simulations over all sim data\n",
    "    for epoch in range(min(n_epochs, len(stats))):\n",
    "        \n",
    "        start = time()\n",
    "        \n",
    "        # set stats\n",
    "        stat = stats.pop(0)\n",
    "        model.set_sim(stat)\n",
    "        \n",
    "        # run\n",
    "        print(\"[Epoch {}]\".format(epoch), end=\"\")\n",
    "        model.partial_fit(train_set)\n",
    "        score = model.score(test_set)\n",
    "        \n",
    "        # store data\n",
    "        datum = {\"epoch\": epoch, \"score\": score, **model.get_params(), **model.meta_}\n",
    "        print(\" Score: {} in {} seconds\".format(score, time() - start))\n",
    "        history.append(datum)\n",
    "        \n",
    "        if epoch % log_interval == 0:\n",
    "            write_stats(history, stats_dir, 'results-ep{}.csv'.format(epoch))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sim(stats_df_loc, out_folder, timings, n_epochs=200, grads_per_worker=128, max_bs=99999999):\n",
    "    \"\"\"\n",
    "    Sets up the simulator model, transforms stats (to account for changes in the base classifier and how\n",
    "    it saves data), and calls the train function.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Running simulation....\\n\\n\")\n",
    "    start = time()\n",
    "    \n",
    "    # stats from an actual training run\n",
    "    stats_df = pd.read_csv(stats_df_loc)\n",
    "    \n",
    "    # old versions of the DaskClassifier had different collumn names\n",
    "    if 'lr_' in stats_df.columns:\n",
    "        stats_df = stats_df.rename(columns={\"lr_\": \"partial_fit__lr\", \"batch_size_\": \"partial_fit__batch_size\"})\n",
    "    \n",
    "    # pull initial stats from the CSV\n",
    "    init_lr = stats_df['partial_fit__lr'][0]\n",
    "    init_bs = stats_df['partial_fit__batch_size'][0]\n",
    "    momentum = stats_df['optimizer__momentum'][0]\n",
    "    device = \"cpu\" if not torch.cuda.is_available() else \"cuda:0\"\n",
    "\n",
    "    # args\n",
    "    args = dict(\n",
    "        module=Wide_ResNet,\n",
    "        module__depth=16,\n",
    "        module__widen_factor=4,\n",
    "        module__dropout_rate=0.3,\n",
    "        module__num_classes=len(classes),\n",
    "        loss=torch.nn.CrossEntropyLoss,\n",
    "        optimizer=torch.optim.SGD,\n",
    "        optimizer__lr=init_lr,\n",
    "        optimizer__momentum=momentum,\n",
    "        optimizer__nesterov=True,\n",
    "        optimizer__weight_decay=0.5e-3,\n",
    "        batch_size=init_bs,\n",
    "        max_epochs=200,\n",
    "        device=device,\n",
    "        grads_per_worker=grads_per_worker,\n",
    "        client=client,\n",
    "        lr=init_lr,\n",
    "        max_batch_size=max_bs\n",
    "    )\n",
    "    \n",
    "    # create model and set initial timings\n",
    "    model = DaskClassifierSimulator(**args)\n",
    "    model.set_times(timings['mult'], timings['score'], timings['deepcopy'], timings['grad128'])\n",
    "    \n",
    "    # train based on normal train function\n",
    "    hist = train(model, \n",
    "             train_set, \n",
    "             test_set, \n",
    "             stats_df,\n",
    "             n_epochs=n_epochs, \n",
    "             log_interval=20,\n",
    "             stats_dir=out_folder\n",
    "            )\n",
    "    write_stats(hist, out_folder, 'results-final.csv')\n",
    "    \n",
    "    print('Finished simulation in {} seconds'.format(time() - start))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test exp\n",
    "timings = { 'mult': True, 'score': 0.0, 'deepcopy': 0.0, 'grad128': 0.0 }\n",
    "stats_path = './exp-dask/stats/increasing-bs/exp-final.csv'\n",
    "out_path = 'sim-results/test'\n",
    "run_sim(stats_path, out_path, timings, n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siming grads per worker\n",
    "Simulate the bs=512 experiments with differen numbers of grads per worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Timings for new simulation\n",
    "SCORE_TIME = 0.0 # 5.39407711148262\n",
    "DEEPCOPY_TIME = 1.55e-3 * 2  # seconds\n",
    "GRAD_TIME_128 = 78.32e-3 \n",
    "timings = { 'mult': True, 'score': SCORE_TIME, 'deepcopy': DEEPCOPY_TIME, 'grad128': GRAD_TIME_128 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_path = './exp-dask/stats/dec-lr-512bs/exp-final.csv'\n",
    "for grads_per_worker in [32, 64, 128, 256, 512]:\n",
    "    out_path = 'sim-results/grads_per_worker_{}_test'.format(grads_per_worker)\n",
    "    run_sim(stats_path, out_path, timings, n_epochs=200, grads_per_worker=grads_per_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations with faster network bandwidth #2 (comment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Timings for new simulation\n",
    "SCORE_TIME = 0.0 # 5.39407711148262\n",
    "DEEPCOPY_TIME = 0.127488e-3  # seconds\n",
    "GRAD_TIME_128 = 78.32e-3  # seconds\n",
    "timings = { 'mult': True, 'score': SCORE_TIME, 'deepcopy': DEEPCOPY_TIME, 'grad128': GRAD_TIME_128 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 2.1 - dec-lr-P-machine-fast\n",
    "stats_path = './exp-dask/stats/decreasing-lr/exp--final.csv'\n",
    "out_path = 'sim-results/dec-lr-mult-machines-fast'\n",
    "run_sim(stats_path, out_path, timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 2.2 - increasing batch p machines\n",
    "stats_path = './exp-dask/stats/increasing-bs/exp-final.csv'\n",
    "out_path = 'sim-results/inc-bs-mult-machines-fast'\n",
    "run_sim(stats_path, out_path, timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 2.3 - hybrid 1 P machine\n",
    "stats_path = './exp-dask/stats/hybrid/exp-final.csv'\n",
    "out_path = 'sim-results/hybrid1-mult-machines-fast'\n",
    "run_sim(stats_path, out_path, timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 2.4 - hybrid 2 P machine\n",
    "stats_path = './exp-dask/stats/hybrid-2/exp-final.csv'\n",
    "out_path = 'sim-results/hybrid2-mult-machines-fast'\n",
    "run_sim(stats_path, out_path, timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 11 - large bs 1 P machine\n",
    "stats_path = './exp-dask/stats/large-bs-0/exp-final.csv'\n",
    "out_path = 'sim-results/large-bs1-mult-machines-fast'\n",
    "run_sim(stats_path, out_path, timings, max_bs=5120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 12 - large bs 2 P machine\n",
    "stats_path = './exp-dask/stats/large-bs-1/exp-final.csv'\n",
    "out_path = 'sim-results/large-bs2-mult-machines-fast'\n",
    "run_sim(stats_path, out_path, timings, max_bs=5120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Simulations for 1 machine with normal network bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TIMINGS FOR NORMAL EXPERIMENTS\n",
    "SCORE_TIME = 0.0 # 5.39407711148262\n",
    "DEEPCOPY_TIME = 0.05855  # seconds\n",
    "GRAD_TIME_128 = 0.07832  # seconds\n",
    "timings = { 'mult': False, 'score': SCORE_TIME, 'deepcopy': DEEPCOPY_TIME, 'grad128': GRAD_TIME_128 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 1 - dec-lr-1-machine \n",
    "stats_path = './exp-dask/stats/decreasing-lr/exp--final.csv'\n",
    "out_path = 'sim-results/dec-lr-1-machine'\n",
    "run_sim(stats_path, out_path, timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 5 - large bs 1 1 machine\n",
    "stats_path = './exp-dask/stats/large-bs-0/exp-final.csv'\n",
    "out_path = 'sim-results/large-bs1-1-machine'\n",
    "run_sim(stats_path, out_path, timings, max_bs=5120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 2 - inc-bs\n",
    "stats_path = './exp-dask/stats/increasing-bs/exp-final.csv'\n",
    "out_path = 'sim-results/inc-bs-1-machine'\n",
    "run_sim(stats_path, out_path, timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 3 - hybrid 1 1 machine\n",
    "stats_path = './exp-dask/stats/hybrid/exp-final.csv'\n",
    "out_path = 'sim-results/hybrid1-1-machine'\n",
    "run_sim(stats_path, out_path, timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 4 - hybrid 2 1 machine\n",
    "stats_path = './exp-dask/stats/hybrid-2/exp-final.csv'\n",
    "out_path = 'sim-results/hybrid2-1-machine'\n",
    "run_sim(stats_path, out_path, timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 6 - large bs 2 1 machine\n",
    "stats_path = './exp-dask/stats/large-bs-1/exp-final.csv'\n",
    "out_path = 'sim-results/large-bs2-1-machine'\n",
    "run_sim(stats_path, out_path, timings, max_bs=5120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations for multiple machines with normal network bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timings for multi machine set ups\n",
    "SCORE_TIME = 0.0 # 5.39407711148262\n",
    "DEEPCOPY_TIME = 0.0 # set internall based on N workers\n",
    "GRAD_TIME_128 = 0.07832  # seconds\n",
    "timings = { 'mult': True, 'score': SCORE_TIME, 'deepcopy': DEEPCOPY_TIME, 'grad128': GRAD_TIME_128 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 11 - large bs 1 P machine\n",
    "stats_path = './exp-dask/stats/large-bs-0/exp-final.csv'\n",
    "out_path = 'sim-results/large-bs1-mult-machines'\n",
    "run_sim(stats_path, out_path, timings, max_bs=5120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 7 - dec-lr-P-machine \n",
    "stats_path = './exp-dask/stats/decreasing-lr/exp--final.csv'\n",
    "out_path = 'sim-results/dec-lr-mult-machines'\n",
    "run_sim(stats_path, out_path, timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 8 - increasing batch p machines\n",
    "stats_path = './exp-dask/stats/increasing-bs/exp-final.csv'\n",
    "out_path = 'sim-results/inc-bs-mult-machines'\n",
    "run_sim(stats_path, out_path, timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 9 - hybrid 1 P machine\n",
    "stats_path = './exp-dask/stats/hybrid/exp-final.csv'\n",
    "out_path = 'sim-results/hybrid1-mult-machines'\n",
    "run_sim(stats_path, out_path, timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 10 - hybrid 2 P machine\n",
    "stats_path = './exp-dask/stats/hybrid-2/exp-final.csv'\n",
    "out_path = 'sim-results/hybrid2-mult-machines'\n",
    "run_sim(stats_path, out_path, timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 12 - large bs 2 P machine\n",
    "stats_path = './exp-dask/stats/large-bs-1/exp-final.csv'\n",
    "out_path = 'sim-results/large-bs2-mult-machines'\n",
    "run_sim(stats_path, out_path, timings, max_bs=5120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adadamp",
   "language": "python",
   "name": "adadamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
