{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Wide ResNet with Dask Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/ubuntu/adadamp-experiments')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "from copy import copy\n",
    "from adadamp.adadamp import DaskClassifier, DaskClassifierExpiriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>inproc://172.31.40.124/30206/1</li>\n",
       "  <li><b>Dashboard: </b><a href='http://172.31.40.124:8787/status' target='_blank'>http://172.31.40.124:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>16.48 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'inproc://172.31.40.124/30206/1' processes=1 threads=4, memory=16.48 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training client\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import performance_report\n",
    "\n",
    "def _prep():\n",
    "    from distributed.protocol import torch\n",
    "\n",
    "client = Client(processes=False)\n",
    "client.run(_prep)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Wide_ResNet\n",
    "\n",
    "client.upload_file(\"./exp-dask/model.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_set = torchvision.datasets.CIFAR10(root='./exp-dask/data', train=True, download=True, transform=transform_train)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./exp-dask/data', train=False, download=True, transform=transform_test)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for expiriment 1: \n",
    "#    “Decaying learning rate” follows the original implementation; \n",
    "#     the batch size is constant, while the learning rate repeatedly \n",
    "#     decays by a factor of 5 at a sequence of steps\n",
    "# my understanding is that this is the \"control\" expeririment, where we are not touching the number of \n",
    "# workers nor the batch size\n",
    "device = \"cpu\" if not torch.cuda.is_available() else \"cuda:0\"\n",
    "model = DaskClassifierExpiriments(\n",
    "    module=Wide_ResNet,\n",
    "    module__depth=16,\n",
    "    module__widen_factor=4,\n",
    "    module__dropout_rate=0.3,\n",
    "    module__num_classes=len(classes),\n",
    "    loss=nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.SGD,\n",
    "    optimizer__lr=0.1,\n",
    "    optimizer__momentum=0.9,\n",
    "    optimizer__nesterov=True,\n",
    "    optimizer__weight_decay=0.5e-3,\n",
    "    batch_size=128,\n",
    "    max_epochs=200,\n",
    "    device=device,\n",
    "    grads_per_worker=128,\n",
    "    client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0 - broken dist.py, LR 1\n",
    "# v1 - fixed dist.py, LR 1\n",
    "# v2 - LR 0.02\n",
    "# v3 - LR 0.02, decaying learning rate, fixed data load\n",
    "\n",
    "def write_stats(hist, epoch, exp, tag):\n",
    "    toCSV = hist\n",
    "    with open('./exp-dask/stats/{}/exp-{}-ep{}.csv'.format(exp, tag, epoch), 'w', encoding='utf8', newline='') as output_file:\n",
    "        fc = csv.DictWriter(output_file, fieldnames=toCSV[0].keys())\n",
    "        fc.writeheader()\n",
    "        fc.writerows(toCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model, test_set):\n",
    "    \"\"\"\n",
    "    tests if model meets certain testing standards:\n",
    "    - by 10th epoch, accuracy is over 70%\n",
    "    \"\"\"\n",
    "    if epoch == 10:\n",
    "        print(\"[TEST] Testing accuracy for 10th epoch is over 70%\")\n",
    "        score = model.score(test_set)\n",
    "        acc = model._meta['score__acc']\n",
    "        if acc < 0.70:\n",
    "            print(\"[TEST] Test failed with {} accuracy\".format(acc))\n",
    "            return False\n",
    "        else:\n",
    "            print(\"[TEST] Test passed with {} accuracy\".format(acc))\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, test_set, n_epochs=200, epoch_sched=[], lr_sched=[], bs_sched=[], log_interval=1, exp='increasing-bs'):\n",
    "    \"\"\"\n",
    "    Train based on expiriment params\n",
    "    \n",
    "    Parameters:\n",
    "    epoch_sched: update lr and bs at epochs in this list\n",
    "    lr_sched: update lr to value at matching epoch. Should be same length as epoch_sched\n",
    "    bs_sched: update bs to value at matching epoch. Should be same length as epoch_sched\n",
    "    \"\"\"\n",
    "    assert len(epoch_sched) == len(lr_sched) == len(bs_sched), \"Invalid schedules. Epoch, lr and bs schedules should all be the same length.\"\n",
    "    \n",
    "    epochs = copy(epoch_sched)\n",
    "    lrs = copy(lr_sched)\n",
    "    bss = copy(bs_sched)\n",
    "    \n",
    "    history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        # check for updates\n",
    "        if len(epochs) > 0 and epochs[0] == epoch:\n",
    "            lr = lrs.pop(0)\n",
    "            bs = bss.pop(0)\n",
    "            epochs.pop(0)\n",
    "            model.set_lr(lr)\n",
    "            model.set_bs(bs)\n",
    "            print(\"[UPDATE] Updated model params:\\n\\tlr: {}\\n\\tbs: {}\".format(lr, bs))\n",
    "            if epoch != 0:\n",
    "                print(\"[UPDATE] Running loss on train set...\", end=\" \")\n",
    "                train_score = model.score(train_set)\n",
    "                print(model._meta['score__loss'], '(acc:', model._meta['score__acc'], ')')\n",
    "        # run\n",
    "        print(\"[Epoch {}]\".format(epoch), end=\"\")\n",
    "        model.partial_fit(train_set)\n",
    "        score = model.score(test_set)\n",
    "        datum = {\"epoch\": epoch, \"score\": score, **model.get_params(), **model.meta_}\n",
    "        print(\" Score: {}\".format(score))\n",
    "        history.append(datum)\n",
    "        \n",
    "        # test\n",
    "        if test(epoch, model, test_set) == False:\n",
    "            print(\"[TEST] Test failed, exiting\")\n",
    "            break\n",
    "        \n",
    "        if epoch % log_interval == 0:\n",
    "            write_stats(history, 'ep{}'.format(epoch), 'increasing-bs', 'v4')\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UPDATE] Updated model params:\n",
      "\tlr: 0.02\n",
      "\tbs: 128\n",
      "[Epoch 0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/adadamp-experiments/adadamp/adadamp/_dist.py:185: UserWarning: Model appears not to update with weight difference {diff}\n",
      "  warn(\"Model appears not to update with weight difference {diff}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Score: 0.5777999758720398\n",
      "[Epoch 1] Score: 0.6837999820709229\n",
      "[Epoch 2] Score: 0.7258999943733215\n",
      "[Epoch 3] Score: 0.7669000029563904\n",
      "[Epoch 4] Score: 0.7827999591827393\n",
      "[Epoch 5] Score: 0.8070999979972839\n",
      "[Epoch 6] Score: 0.8023999929428101\n",
      "[Epoch 7] Score: 0.8172999620437622\n",
      "[Epoch 8] Score: 0.8348999619483948\n",
      "[Epoch 9] Score: 0.8319000005722046\n",
      "[Epoch 10] Score: 0.8432999849319458\n",
      "[TEST] Testing accuracy for 10th epoch is over 70%\n",
      "[TEST] Test passed with 0.8450999855995178 accuracy\n",
      "[Epoch 11] Score: 0.8392999768257141\n",
      "[Epoch 12] Score: 0.8456999659538269\n",
      "[Epoch 13] Score: 0.8502999544143677\n",
      "[Epoch 14] Score: 0.8495000004768372\n",
      "[Epoch 15] Score: 0.852899968624115\n",
      "[Epoch 16] Score: 0.8586999773979187\n",
      "[Epoch 17] Score: 0.8610999584197998\n",
      "[Epoch 18] Score: 0.8637999892234802\n",
      "[Epoch 19] Score: 0.8703999519348145\n",
      "[Epoch 20] Score: 0.8587999939918518\n",
      "[Epoch 21] Score: 0.864799976348877\n",
      "[Epoch 22] Score: 0.8640999794006348\n",
      "[Epoch 23] Score: 0.8718000054359436\n",
      "[Epoch 24] Score: 0.8687999844551086\n",
      "[Epoch 25] Score: 0.8671999573707581\n",
      "[Epoch 26] Score: 0.870199978351593\n",
      "[Epoch 27] Score: 0.8698999881744385\n",
      "[Epoch 28] Score: 0.8765999674797058\n",
      "[Epoch 29] Score: 0.8715999722480774\n",
      "[Epoch 30] Score: 0.873199999332428\n",
      "[Epoch 31] Score: 0.8738999962806702\n",
      "[Epoch 32] Score: 0.8769999742507935\n",
      "[Epoch 33] Score: 0.8718999624252319\n",
      "[Epoch 34] Score: 0.8704999685287476\n",
      "[Epoch 35] Score: 0.8793999552726746\n",
      "[Epoch 36] Score: 0.8787999749183655\n",
      "[Epoch 37] Score: 0.8777999877929688\n",
      "[Epoch 38] Score: 0.8786999583244324\n",
      "[Epoch 39] Score: 0.8759999871253967\n",
      "[Epoch 40] Score: 0.8860999941825867\n",
      "[Epoch 41] Score: 0.8824999928474426\n",
      "[Epoch 42] Score: 0.8872999548912048\n",
      "[Epoch 43] Score: 0.8691999912261963\n",
      "[Epoch 44] Score: 0.8853999972343445\n",
      "[Epoch 45] Score: 0.8788999915122986\n",
      "[Epoch 46] Score: 0.8704999685287476\n",
      "[Epoch 47] Score: 0.8877999782562256\n",
      "[Epoch 48] Score: 0.8840000033378601\n",
      "[Epoch 49] Score: 0.8783999681472778\n",
      "[Epoch 50] Score: 0.8841999769210815\n",
      "[Epoch 51] Score: 0.8881999850273132\n",
      "[Epoch 52] Score: 0.8892999887466431\n",
      "[Epoch 53] Score: 0.8816999793052673\n",
      "[Epoch 54] Score: 0.887999951839447\n",
      "[Epoch 55] Score: 0.8823999762535095\n",
      "[Epoch 56] Score: 0.8829999566078186\n",
      "[Epoch 57] Score: 0.8809999823570251\n",
      "[Epoch 58] Score: 0.8806999921798706\n",
      "[Epoch 59] Score: 0.8777999877929688\n",
      "[UPDATE] Updated model params:\n",
      "\tlr: 0.004\n",
      "\tbs: 128\n",
      "[UPDATE] Running loss on train set... 0.15744106384277343 (acc: 0.9461199641227722 )\n",
      "[Epoch 60] Score: 0.9113999605178833\n",
      "[Epoch 61] Score: 0.9084999561309814\n",
      "[Epoch 62] Score: 0.9128999710083008\n",
      "[Epoch 63] Score: 0.9156000018119812\n",
      "[Epoch 64] Score: 0.9138999581336975\n",
      "[Epoch 65] Score: 0.9149999618530273\n",
      "[Epoch 66] Score: 0.9140999913215637\n",
      "[Epoch 67] Score: 0.9146999716758728\n",
      "[Epoch 68] Score: 0.9185999631881714\n",
      "[Epoch 69] Score: 0.9152999520301819\n",
      "[Epoch 70] Score: 0.9192999601364136\n",
      "[Epoch 71] Score: 0.9165999889373779\n",
      "[Epoch 72] Score: 0.913100004196167\n",
      "[Epoch 73] Score: 0.9176999926567078\n",
      "[Epoch 74] Score: 0.9177999496459961\n",
      "[Epoch 75] Score: 0.909500002861023\n",
      "[Epoch 76] Score: 0.9156000018119812\n",
      "[Epoch 77] Score: 0.9161999821662903\n",
      "[Epoch 78] Score: 0.9150999784469604\n",
      "[Epoch 79] Score: 0.9182999730110168\n",
      "[Epoch 80] Score: 0.9126999974250793\n",
      "[Epoch 81] Score: 0.9154999852180481\n",
      "[Epoch 82] Score: 0.916700005531311\n",
      "[Epoch 83] Score: 0.9132999777793884\n",
      "[Epoch 84] Score: 0.9122999906539917\n",
      "[Epoch 85] Score: 0.913100004196167\n",
      "[Epoch 86] Score: 0.9170999526977539\n",
      "[Epoch 87] Score: 0.9115999937057495\n",
      "[Epoch 88] Score: 0.9140999913215637\n",
      "[Epoch 89] Score: 0.9104999899864197\n",
      "[Epoch 90] Score: 0.9107999801635742\n",
      "[Epoch 91] Score: 0.9098999500274658\n",
      "[Epoch 92] Score: 0.9124999642372131\n",
      "[Epoch 93] Score: 0.9125999808311462\n",
      "[Epoch 94] Score: 0.9140999913215637\n",
      "[Epoch 95] Score: 0.916700005531311\n",
      "[Epoch 96] Score: 0.9127999544143677\n",
      "[Epoch 97] Score: 0.9110999703407288\n",
      "[Epoch 98] Score: 0.911799967288971\n",
      "[Epoch 99] Score: 0.9115999937057495\n",
      "[Epoch 100] Score: 0.9160999655723572\n",
      "[Epoch 101] Score: 0.9116999506950378\n",
      "[Epoch 102] Score: 0.9101999998092651\n",
      "[Epoch 103] Score: 0.9103999733924866\n",
      "[Epoch 104] Score: 0.9101999998092651\n",
      "[Epoch 105] Score: 0.9140999913215637\n",
      "[Epoch 106] Score: 0.9124999642372131\n",
      "[Epoch 107] Score: 0.910099983215332\n",
      "[Epoch 108] Score: 0.9055999517440796\n",
      "[Epoch 109] Score: 0.9101999998092651\n",
      "[Epoch 110] Score: 0.9084999561309814\n",
      "[Epoch 111] Score: 0.913599967956543\n",
      "[Epoch 112] Score: 0.9162999987602234\n",
      "[Epoch 113] Score: 0.9103999733924866\n",
      "[Epoch 114] Score: 0.9083999991416931\n",
      "[Epoch 115] Score: 0.913599967956543\n",
      "[Epoch 116] Score: 0.9084999561309814\n",
      "[Epoch 117] Score: 0.905299961566925\n",
      "[Epoch 118] Score: 0.9068999886512756\n",
      "[Epoch 119] Score: 0.9093999862670898\n",
      "[UPDATE] Updated model params:\n",
      "\tlr: 0.0008\n",
      "\tbs: 128\n",
      "[UPDATE] Running loss on train set... 0.03140414390563965 (acc: 0.9912399649620056 )\n",
      "[Epoch 120] Score: 0.9163999557495117\n",
      "[Epoch 121] Score: 0.9195999503135681\n",
      "[Epoch 122] Score: 0.920799970626831\n",
      "[Epoch 123] Score: 0.9230999946594238\n",
      "[Epoch 124] Score: 0.920199990272522\n",
      "[Epoch 125] Score: 0.9212999939918518\n",
      "[Epoch 126] Score: 0.9200999736785889\n",
      "[Epoch 127] Score: 0.9174999594688416\n",
      "[Epoch 128] Score: 0.9241999983787537\n",
      "[Epoch 129] Score: 0.920699954032898\n",
      "[Epoch 130] Score: 0.9205999970436096\n",
      "[Epoch 131] Score: 0.9202999472618103\n",
      "[Epoch 132] Score: 0.9176999926567078\n",
      "[Epoch 133] Score: 0.920799970626831\n",
      "[Epoch 134] Score: 0.918999969959259\n",
      "[Epoch 135] Score: 0.9167999625205994\n",
      "[Epoch 136] Score: 0.9197999835014343\n",
      "[Epoch 137] Score: 0.9208999872207642\n",
      "[Epoch 138] Score: 0.920199990272522\n",
      "[Epoch 139] Score: 0.9194999933242798\n",
      "[Epoch 140] Score: 0.9205999970436096\n",
      "[Epoch 141] Score: 0.920699954032898\n",
      "[Epoch 142] Score: 0.9222999811172485\n",
      "[Epoch 143] Score: 0.9199999570846558\n",
      "[Epoch 144] Score: 0.9195999503135681\n",
      "[Epoch 145] Score: 0.9217999577522278\n",
      "[Epoch 146] Score: 0.9229999780654907\n",
      "[Epoch 147] Score: 0.9236999750137329\n",
      "[Epoch 148] Score: 0.9210999608039856\n",
      "[Epoch 149] Score: 0.9205999970436096\n",
      "[Epoch 150] Score: 0.9192999601364136\n",
      "[Epoch 151] Score: 0.9225999712944031\n",
      "[Epoch 152] Score: 0.9190999865531921\n",
      "[Epoch 153] Score: 0.9225999712944031\n",
      "[Epoch 154] Score: 0.9200999736785889\n",
      "[Epoch 155] Score: 0.9210000038146973\n",
      "[Epoch 156] Score: 0.920799970626831\n",
      "[Epoch 157] Score: 0.9190999865531921\n",
      "[Epoch 158] Score: 0.9208999872207642\n",
      "[Epoch 159] Score: 0.9199000000953674\n",
      "[Epoch 160] Score: 0.9218999743461609\n",
      "[Epoch 161] Score: 0.9192000031471252\n",
      "[Epoch 162] Score: 0.9215999841690063\n",
      "[Epoch 163] Score: 0.920699954032898\n",
      "[Epoch 164] Score: 0.918999969959259\n",
      "[Epoch 165] Score: 0.9236999750137329\n",
      "[Epoch 166] Score: 0.9179999828338623\n",
      "[Epoch 167] Score: 0.9222999811172485\n",
      "[Epoch 168] Score: 0.9232999682426453\n",
      "[Epoch 169] Score: 0.9199000000953674\n",
      "[Epoch 170] Score: 0.9215999841690063\n",
      "[Epoch 171] Score: 0.9175999760627747\n",
      "[Epoch 172] Score: 0.9193999767303467\n",
      "[Epoch 173] Score: 0.9192000031471252\n",
      "[Epoch 174] Score: 0.9211999773979187\n",
      "[Epoch 175] Score: 0.918999969959259\n",
      "[Epoch 176] Score: 0.9217000007629395\n",
      "[Epoch 177] Score: 0.918999969959259\n",
      "[Epoch 178] Score: 0.9228999614715576\n",
      "[Epoch 179] Score: 0.9192000031471252\n",
      "[UPDATE] Updated model params:\n",
      "\tlr: 0.00016\n",
      "\tbs: 128\n",
      "[UPDATE] Running loss on train set... 0.005274621901512146 (acc: 0.9996399879455566 )\n",
      "[Epoch 180] Score: 0.9240999817848206\n",
      "[Epoch 181] Score: 0.9205999970436096\n",
      "[Epoch 182] Score: 0.9226999878883362\n",
      "[Epoch 183] Score: 0.9194999933242798\n",
      "[Epoch 184] Score: 0.9233999848365784\n",
      "[Epoch 185] Score: 0.9215999841690063\n",
      "[Epoch 186] Score: 0.9211999773979187\n",
      "[Epoch 187] Score: 0.9194999933242798\n",
      "[Epoch 188] Score: 0.9233999848365784\n",
      "[Epoch 189] Score: 0.9192999601364136\n",
      "[Epoch 190] Score: 0.9215999841690063\n",
      "[Epoch 191] Score: 0.920199990272522\n",
      "[Epoch 192] Score: 0.9187999963760376\n",
      "[Epoch 193] Score: 0.9218999743461609\n",
      "[Epoch 194] Score: 0.9228999614715576\n",
      "[Epoch 195] Score: 0.9226999878883362\n",
      "[Epoch 196] Score: 0.920199990272522\n",
      "[Epoch 197] Score: 0.9200999736785889\n",
      "[Epoch 198] Score: 0.9215999841690063\n",
      "[Epoch 199] Score: 0.9218999743461609\n"
     ]
    }
   ],
   "source": [
    "# ie \"Update LR to 0.1 and bs t0 640 on 60th epoch\"\n",
    "exp0_epochs = [0, 60, 120, 180]\n",
    "exp0_lr = [\n",
    "    0.02, \n",
    "    0.02 / 5, \n",
    "    0.02 / 5 / 5, \n",
    "    0.02 / 5 / 5 / 5\n",
    "]\n",
    "exp0_bs = [128, 128, 128, 128]\n",
    "# train\n",
    "hist = None\n",
    "with performance_report(filename=\"dask-report.html\"):\n",
    "    hist = train(model, \n",
    "             train_set, \n",
    "             test_set, \n",
    "             n_epochs=200, \n",
    "             epoch_sched=exp0_epochs, \n",
    "             lr_sched=exp0_lr, \n",
    "             bs_sched=exp0_bs, \n",
    "             log_interval=20,\n",
    "             exp='increasing-bs'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_stats(hist, '-final', 'increasing-bs', 'v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ie \"Update LR to 0.1 and bs t0 640 on 60th epoch\"\n",
    "exp1_epochs = [0, 60, 120, 180]\n",
    "exp1_lr = [0.02, 0.02, 0.02, 0.02]\n",
    "exp1_bs = [128, 640, 3200, 16000]\n",
    "# train\n",
    "hist = train(model, \n",
    "             train_set, \n",
    "             test_set, \n",
    "             n_epochs=200, \n",
    "             epoch_sched=exp1_epochs, \n",
    "             lr_sched=exp1_lr, \n",
    "             bs_sched=exp1_bs, \n",
    "             log_interval=20,\n",
    "             exp='increasing-bs'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ie \"Update LR to 0.1 and bs t0 640 on 60th epoch\"\n",
    "exp2_epochs = [0, 60, 120, 180]\n",
    "exp2_lr = [0.1, \n",
    "           0.1, \n",
    "           0.1 / 5, \n",
    "           0.1 / 5 / 5\n",
    "          ]\n",
    "exp2_bs = [128, 640, 640, 640]\n",
    "# train\n",
    "hist = train(model, train_set, test_set, n_epochs=200, epoch_sched=exp1_epochs, lr_sched=exp1_lr, bs_sched=exp1_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ie \"Update LR to 0.1 and bs t0 640 on 60th epoch\"\n",
    "exp3_epochs = [0, 60, 120, 180]\n",
    "exp3_lr = [0.1, \n",
    "           0.1 / 5, \n",
    "           0.1 / 5 / 5,\n",
    "           0.1 / 5 / 5 / 5\n",
    "          ]\n",
    "exp3_bs = [128, 128, 128, 128]\n",
    "# train\n",
    "hist = train(model, train_set, test_set, n_epochs=200, epoch_sched=exp1_epochs, lr_sched=exp1_lr, bs_sched=exp1_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toCSV = model.curr_metas\n",
    "with open('./exp-dask/exp1-v0.csv', 'w', encoding='utf8', newline='') as output_file:\n",
    "    fc = csv.DictWriter(output_file, fieldnames=toCSV[0].keys())\n",
    "    fc.writeheader()\n",
    "    fc.writerows(toCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
