{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains model. Each model has the same initialization, and only differs in the optimizer used (GeoDamp, GeoDamp LR, PadaDamp, PadaDamp LR, or Adagrad).\n",
    "\n",
    "* Take in hyperparameters from `Tune.ipynb`.\n",
    "* Initialize models with those hyperparameters;\n",
    "    * (note: the same initialization is used for all optimizers).\n",
    "* Train those models, record the test statistics, and save the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tune import PadaDamp, GeoDamp, GeoDampLR, Damper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "For continuous features, make them have unit variance and 0 mean. For the binary features, don't do preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize(X, scale, cnts=10):\n",
    "    for c in range(X.shape[1]):\n",
    "        if c < cnts:\n",
    "            assert len(np.unique(X[:, c])) > 2\n",
    "        else:\n",
    "            assert 1 <= len(np.unique(X[:, c])) <= 2\n",
    "    Y = scale.transform(X[:, :cnts])\n",
    "    assert len(np.unique(Y[:, -1])) > 2\n",
    "\n",
    "    Z = np.hstack((Y, X[:, cnts:]))\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(damper):\n",
    "    X, y = fetch_covtype(return_X_y=True, random_state=2, shuffle=True, data_home=f\"~/scikit_learn_data/{damper}/\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, random_state=42, train_size=25_000\n",
    "    )\n",
    "    cnts = 10\n",
    "    scale = StandardScaler().fit(X_train[:, :cnts])\n",
    "\n",
    "    X_train = normalize(X_train, scale, cnts=cnts)\n",
    "    X_test = normalize(X_test, scale, cnts=cnts)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Train on the entire train set, and test on testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(damper, max_iter=200):\n",
    "    X_train, y_train, X_test, y_test = get_train_test_data(type(damper).__name__)\n",
    "    damper.initialize()\n",
    "    test_score =  damper.score(X_test, y_test, return_dict=True, prefix=\"test_\")\n",
    "    train_score =  damper.score(X_train, y_train, return_dict=True, prefix=\"train_\")\n",
    "    meta = {\n",
    "        \"train_eg\": len(y_train),\n",
    "        \"test_eg\": len(y_test),\n",
    "        \"max_iter\": max_iter,\n",
    "        \"damper_name\": type(damper).__name__.lower(),\n",
    "        **damper.get_params(),\n",
    "    }\n",
    "    data = [{\"partial_fit_calls\": 0, **test_score, **train_score, **meta, **copy(damper.meta_)}]\n",
    "    pprint({k: data[-1][k] for k in [\"test_score\", \"train_score\", \"test_loss\", \"train_loss\"]})\n",
    "    for k in range(max_iter):\n",
    "        damper.partial_fit(X_train, y_train)\n",
    "        test_score =  damper.score(X_test, y_test, return_dict=True, prefix=\"test_\")\n",
    "        train_score =  damper.score(X_train, y_train, return_dict=True, prefix=\"train_\")\n",
    "        datum = {\n",
    "            \"partial_fit_calls\": k + 1,\n",
    "            **meta,\n",
    "            **test_score,\n",
    "            **train_score,\n",
    "            **copy(damper.meta_)\n",
    "        }\n",
    "        cols = [\"damper_name\", \"partial_fit_calls\", \"test_score\", \"train_score\", \"batch_size\", \"lr_\"]\n",
    "        show = {k: datum[k]\n",
    "                for k in cols\n",
    "                if k in datum\n",
    "               }\n",
    "        show[\"epochs\"] = datum[\"num_examples\"] / meta[\"train_eg\"]\n",
    "        pprint(show)\n",
    "        data.append(datum)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters were found from Tune.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padadamp_params = {\n",
    "    'batch_growth_rate': 0.0020999350113148425,\n",
    "    'dwell': 500,\n",
    "    'initial_batch_size': 64,\n",
    "    'lr': 0.0044576595887146865,\n",
    "    'max_batch_size': 2048,\n",
    "    'momentum': 0.8924479261291679,\n",
    "    'weight_decay': 0.00012906483459130378\n",
    "}\n",
    "\n",
    "geodamp_params =  {\n",
    "    'dampingdelay': 48.66414550165989,\n",
    "    'dampingfactor': 4.283407572527471,\n",
    "    'initial_batch_size': 32,\n",
    "    'lr': 0.004618645820872018,\n",
    "    'max_batch_size': 256,\n",
    "    'momentum': 0.7320904056485099,\n",
    "    'weight_decay': 0.001122178654655193\n",
    "}\n",
    "\n",
    "geodamplr_params = copy(geodamp_params)\n",
    "padadamplr_params = copy(padadamp_params)\n",
    "static_bs = 64\n",
    "geodamplr_params = {\n",
    "    k: v\n",
    "    for k, v in geodamp_params.items()\n",
    "    if k not in [\"initial_batch_size\", \"max_batch_size\"]\n",
    "}\n",
    "geodamplr_params[\"static_batch_size\"] = static_bs\n",
    "padadamplr_params[\"initial_batch_size\"] = static_bs\n",
    "padadamplr_params[\"max_batch_size\"] = static_bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each optimizer, have the same initialization with a `seed=33`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = dict(seed=33)\n",
    "padadamp = PadaDamp(**common, **padadamp_params)\n",
    "geodamp = GeoDamp(**common, **geodamp_params)\n",
    "geodamplr = GeoDampLR(**common, **geodamplr_params)\n",
    "padadamplr = PadaDamp(**common, **padadamplr_params)\n",
    "adagrad = Damper(**common, opt=\"adagrad\", max_batch_size=static_bs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client, as_completed\n",
    "client = Client()\n",
    "\n",
    "# If a Dask scheduler is at localhost:8786. Perform this by running these commands:\n",
    "#\n",
    "#     $ dask-scheduler\n",
    "#     $ dask-worker --nprocs 20 --nthreads 2 localhost:8786\n",
    "#\n",
    "# client = Client(\"localhost:8786\")\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dampers = [padadamp, padadamplr, geodamp, geodamplr, adagrad]\n",
    "futures = client.map(train, dampers, max_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, future in enumerate(as_completed(futures)):\n",
    "    data = future.result()\n",
    "    print(k)\n",
    "    pprint(data[-1])\n",
    "    pd.DataFrame(data).to_csv(f\"{k}-test-data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:skorch]",
   "language": "python",
   "name": "conda-env-skorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
